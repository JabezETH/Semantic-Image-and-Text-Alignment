{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access your OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def json_to_image(image_path: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Converts an image file to a PIL Image object.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image.Image: Image as a PIL Image object.\n",
    "    \"\"\"\n",
    "    return Image.open(image_path)\n",
    "\n",
    "def read_img(json_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads an image path from a JSON file, converts both a constant image and the image from JSON to JPEG, and sends them to the OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    - json_path (str): Path to the JSON file containing the image path.\n",
    "    - constant_json_path (str): Path to the JSON file containing the constant image path.\n",
    "    - openai_api_key (str): OpenAI API key for authentication.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Response from the OpenAI API.\n",
    "    \"\"\"\n",
    "    constant_json_path = \"/home/jabez/week_12/Semantic-Image-and-Text-Alignment/notebooks/output/constant_image_json.json\"\n",
    "    # Load JSON data from files\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    with open(constant_json_path, 'r') as constant_json_file:\n",
    "        constant_json_data = json.load(constant_json_file)\n",
    "    \n",
    "    # Extract the image paths from the JSON data\n",
    "    image_path = json_data['output_path']\n",
    "    constant_image_path = constant_json_data['constant_image_path']\n",
    "\n",
    "    # Convert image files to PIL Images\n",
    "    constant_image = json_to_image(constant_image_path).convert(\"RGB\")\n",
    "    image = json_to_image(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Convert images to base64 strings in JPEG format\n",
    "    buffered1 = BytesIO()\n",
    "    constant_image.save(buffered1, format=\"JPEG\")\n",
    "    base64_constant_image = base64.b64encode(buffered1.getvalue()).decode('utf-8')\n",
    "\n",
    "    buffered2 = BytesIO()\n",
    "    image.save(buffered2, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(buffered2.getvalue()).decode('utf-8')\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"compare the two image only object positions, brightness, and layer for each object. First rate 1 to 10 the similarity between image 1 and image 2. and suggest what to change in image 2 to make it like image 1\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_constant_image}\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    response_data = response.json()\n",
    "    return response_data['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "def json_to_image(image_path: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Converts an image file to a PIL Image object.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image.Image: Image as a PIL Image object.\n",
    "    \"\"\"\n",
    "    return Image.open(image_path)\n",
    "\n",
    "def encode_image(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Encodes an image file to a base64 string.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Base64 encoded string of the image.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def describe_image(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Describes an image by sending it to an API and getting the description.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - str: Description of the image.\n",
    "    \"\"\"\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"What’s in this image?\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    response_data = response.json()\n",
    "    return response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "def describe_images_from_json(json_file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Describes images specified in a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - json_file_path (str): Path to the JSON file containing image paths.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing descriptions for the larger image and small images.\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    larger_image_path = data.get('larger_image_path')\n",
    "    small_images_paths = data.get('small_images_paths', [])\n",
    "\n",
    "    # Describe larger image\n",
    "    larger_image_description = describe_image(larger_image_path)\n",
    "\n",
    "    # Describe small images\n",
    "    small_images_descriptions = [\n",
    "        {\n",
    "            \"image_path\": image_path,\n",
    "            \"description\": describe_image(image_path)\n",
    "        }\n",
    "        for image_path in small_images_paths\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"larger_image_description\": larger_image_description,\n",
    "        \"small_images_descriptions\": small_images_descriptions\n",
    "    }\n",
    "\n",
    "# # Path to your JSON file\n",
    "# json_file_path = \"/home/jabez/week_12/Semantic-Image-and-Text-Alignment/notebooks/img_path.json\"\n",
    "\n",
    "# # Describe images from the JSON file\n",
    "# descriptions = describe_images_from_json(json_file_path)\n",
    "# print(descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make the second image more like the first image in terms of object positioning and layout, you can make the following adjustments:\\n\\n1. **Title Placement:**\\n   - Move the \"OFFBLAK Generation T\" title to the top of the image, similar to the first image.\\n\\n2. **Text Layout:**\\n   - Place the marketing text (\"Discover 12...\") in the middle of the image, similar to how the \"Subscribe to...\" text is centered in the first image.\\n\\n3. **Product Presentation:**\\n   - Display a broader selection of products and place them with some spacing around the middle of the image (like the tea packs and boxes in the first image).\\n\\n4. **Call-to-Action Button:**\\n   - Add a clear \"Shop Now\" button at the bottom of the image, as done in the first image.\\n\\n5. **Supplementary Objects:**\\n   - Add some lifestyle items around the products (e.g., a cup of tea, a pair of sunglasses) to create a similar vibe to the first image.\\n\\n6. **Hand/Icon Element:**\\n   - Place an icon or an element to guide the viewer’s attention to the call to action or important products (similar to the pointing hand in the first image).\\n   \\nHere is a suggested positioning guide to follow: \\n\\n- **Top Section (Brand Name):**\\n  - \"OFFBLAK Generation T\" \\n\\n- **Middle Section (Main Message and Products):**\\n  - Main'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = \"./output/output.json\"\n",
    "read_img(json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_position_from_constant(position_name, larger_image, smaller_image):\n",
    "    \"\"\"\n",
    "    Get the (x, y) coordinates for a given position name based on the size of the larger and smaller images.\n",
    "\n",
    "    Parameters:\n",
    "    - position_name (str): Name of the position (e.g., \"top right\").\n",
    "    - larger_image (numpy.ndarray): The larger image.\n",
    "    - smaller_image (numpy.ndarray): The smaller image.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (x, y) coordinates for the given position.\n",
    "    \"\"\"\n",
    "    larger_h, larger_w, _ = larger_image.shape\n",
    "    smaller_h, smaller_w, _ = smaller_image.shape\n",
    "\n",
    "    positions = {\n",
    "        \"top left\": (0, 0),\n",
    "        \"top center\": ((larger_w - smaller_w) // 2, 0),\n",
    "        \"top right\": (larger_w - smaller_w, 0),\n",
    "        \"center left\": (0, (larger_h - smaller_h) // 2),\n",
    "        \"center center\": ((larger_w - smaller_w) // 2, (larger_h - smaller_h) // 2),\n",
    "        \"center right\": (larger_w - smaller_w, (larger_h - smaller_h) // 2),\n",
    "        \"bottom left\": (0, larger_h - smaller_h),\n",
    "        \"bottom center\": ((larger_w - smaller_w) // 2, larger_h - smaller_h),\n",
    "        \"bottom right\": (larger_w - smaller_w, larger_h - smaller_h),\n",
    "    }\n",
    "\n",
    "    return positions.get(position_name, (0, 0))\n",
    "\n",
    "def blend_images(position_names: list, brightness_values: list, layers: list, alpha: float = 0.5, output_dir: str = './output') -> str:\n",
    "    \"\"\"\n",
    "    Blends multiple small images by placing them on top of a larger image at specified positions and saves the blended image.\n",
    "    \n",
    "    Parameters:\n",
    "    - position_names (list of str): List of position names for each small image, e.g., [\"top left\", \"top right\", \"center center\"].\n",
    "    - brightness_values (list of float): List of brightness adjustment factors for each small image.\n",
    "    - layers (list of int): List of layer values from 1 to 6 for each small image to determine the blending order.\n",
    "    - alpha (float): Blending factor for transparency (0.0 to 1.0). Default is 0.5.\n",
    "    - output_dir (str): Directory where the blended image will be saved. Default is './output'.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Path to the JSON file containing the output image information.\n",
    "    \"\"\"\n",
    "    json_file_path = '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/notebooks/img_path.json'\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    larger_image_path = data['larger_image_path']\n",
    "    small_images_paths = data['small_images_paths']\n",
    "\n",
    "    print(f\"Number of small images: {len(small_images_paths)}\")\n",
    "    print(f\"Number of positions: {len(position_names)}\")\n",
    "    print(f\"Number of brightness values: {len(brightness_values)}\")\n",
    "    print(f\"Number of layers: {len(layers)}\")\n",
    "\n",
    "    if len(small_images_paths) != len(position_names):\n",
    "        raise ValueError(\"The number of small images must match the number of position names.\")\n",
    "    \n",
    "    if len(brightness_values) != len(small_images_paths):\n",
    "        raise ValueError(\"The number of brightness values must match the number of small images.\")\n",
    "    \n",
    "    if len(layers) != len(small_images_paths):\n",
    "        raise ValueError(\"The number of layers must match the number of small images.\")\n",
    "\n",
    "    # Load larger image from file path\n",
    "    larger_image = cv2.imread(larger_image_path, cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    # Convert the larger image to RGBA if it is not already\n",
    "    if larger_image.shape[2] != 4:\n",
    "        larger_image = cv2.cvtColor(larger_image, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "    # Create a blank image with the same size as the larger image\n",
    "    blended_image = np.zeros_like(larger_image)\n",
    "    blended_image[:, :, :] = larger_image\n",
    "\n",
    "    # Sort the images by their layer values\n",
    "    sorted_indices = sorted(range(len(layers)), key=lambda k: layers[k])\n",
    "\n",
    "    # Blend each small image at its respective position in the order of layers\n",
    "    for i in sorted_indices:\n",
    "        small_image_path = small_images_paths[i]\n",
    "        position_name = position_names[i]\n",
    "        brightness_value = brightness_values[i]\n",
    "\n",
    "        # Load small image from file path\n",
    "        smaller_image = cv2.imread(small_image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        # Convert the smaller image to RGBA if it is not already\n",
    "        if smaller_image.shape[2] != 4:\n",
    "            smaller_image = cv2.cvtColor(smaller_image, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "        # Resize smaller image if necessary\n",
    "        smaller_h, smaller_w, _ = smaller_image.shape\n",
    "        larger_h, larger_w, _ = larger_image.shape\n",
    "        if smaller_h > larger_h or smaller_w > larger_w:\n",
    "            aspect_ratio = smaller_w / smaller_h\n",
    "            if smaller_h > larger_h:\n",
    "                smaller_h = larger_h\n",
    "                smaller_w = int(smaller_h * aspect_ratio)\n",
    "            if smaller_w > larger_w:\n",
    "                smaller_w = larger_w\n",
    "                smaller_h = int(smaller_w / aspect_ratio)\n",
    "            smaller_image = cv2.resize(smaller_image, (smaller_w, smaller_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Adjust the brightness of the smaller image\n",
    "        hsv = cv2.cvtColor(smaller_image, cv2.COLOR_BGRA2BGR)\n",
    "        hsv = cv2.cvtColor(hsv, cv2.COLOR_BGR2HSV)\n",
    "        hsv[:, :, 2] = cv2.convertScaleAbs(hsv[:, :, 2], alpha=brightness_value)\n",
    "        smaller_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # Add an alpha channel manually\n",
    "        alpha_channel = np.ones((smaller_image.shape[0], smaller_image.shape[1]), dtype=smaller_image.dtype) * 255\n",
    "        smaller_image = cv2.merge((smaller_image, alpha_channel))\n",
    "\n",
    "        # Get position for the smaller image\n",
    "        x_offset, y_offset = get_position_from_constant(position_name, larger_image, smaller_image)\n",
    "\n",
    "        # Overlay the smaller image onto the blended image\n",
    "        for c in range(0, 3):\n",
    "            blended_image[y_offset:y_offset+smaller_h, x_offset:x_offset+smaller_w, c] = (\n",
    "                alpha * smaller_image[:, :, c] + (1 - alpha) * blended_image[y_offset:y_offset+smaller_h, x_offset:x_offset+smaller_w, c]\n",
    "            )\n",
    "        blended_image[y_offset:y_offset+smaller_h, x_offset:x_offset+smaller_w, 3] = smaller_image[:, :, 3]\n",
    "\n",
    "    # Save blended image to output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'blended_image.png')\n",
    "    cv2.imwrite(output_file, blended_image)\n",
    "\n",
    "    # Create JSON response\n",
    "    json_data = {\n",
    "        'output_path': output_file,\n",
    "        'positions': position_names,\n",
    "        'brightness_values': brightness_values,\n",
    "        'layers': layers,\n",
    "        'alpha': alpha\n",
    "    }\n",
    "\n",
    "    # Write JSON to file\n",
    "    json_output_file = os.path.join(output_dir, 'output.json')\n",
    "    with open(json_output_file, 'w') as f:\n",
    "        json.dump(json_data, f, indent=4)\n",
    "    \n",
    "    return json_output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/output.json'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_names = [\"center center\", \"top center\", \"bottom center\"]\n",
    "layers = [\n",
    "1, 6, 5\n",
    "]\n",
    "brightness_values = [1.9, 1.2, 1.5]\n",
    "output_dir = './output'\n",
    "# Call the blend_images function\n",
    "blend_images(position_names, brightness_values, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(positions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config2 = {\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": openai_api_key}]}\n",
    "code_execution_config = {\"use_docker\": False}\n",
    "\n",
    "# Initialize the assistant agent with the given configurations\n",
    "config_list = [\n",
    "    {\"model\": \"gpt-4\", \"api_key\": openai_api_key, \"api_type\": \"openai\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config={\n",
    "        \"temperature\": 0,\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"functions\": [\n",
    "             {\n",
    "                        \"name\": \"blend_images\",\n",
    "                        \"description\": \"use this function to blend the images\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"position_names\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"This is where you will position the blending\"\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"positions_str\"]\n",
    "                        }\n",
    "                        },\n",
    "             {\n",
    "                        \"name\": \"read_img\",\n",
    "                        \"description\": \"use this to read the image blended from blended image\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"output_json\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"description\": \"This is the blended image\"\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"position\"]\n",
    "                        }\n",
    "                        }\n",
    "                        ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "# Define the assistant agent that suggests tool calls.\n",
    "img_blend_assistant = autogen.AssistantAgent(\n",
    "    name=\"img_blend_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\"\"\"You are a helpful AI assistant. \n",
    "The main problems you will be solving include:\n",
    "- suggest different \"positions\", \"brightness\" and \"layer\" to make a good advertising based on the feedback from 'img_critic_assistant'\n",
    "   - First read the images that are initiated using the describe_images_from_json function.\n",
    "    make sure that the images will not overlap\n",
    "    \n",
    "- Your task:\n",
    "    - Considering the the discription for each picture, find a way to position each picture to give good advertising based on the recommendation you got from 'img_critic_assistant'.\n",
    "    - 'TERMINATE' when the image you blend got a rating 8.\n",
    "- The number of small images must match the number of position names, layers and brightness.\n",
    "- Example:\n",
    "position_names = [\"bottom center\", \"top right\", \"center center\", \"bottom left\", \"bottom center\", \"top center\"]\n",
    "layers = [\n",
    "    6,  # Second layer from the top\n",
    "    1,  # Topmost layer\n",
    "    1,  # Third layer\n",
    "    1,  # Fifth layer\n",
    "    1,  # Sixth layer\n",
    "    1   # Fourth layer\n",
    "]\n",
    "brightness_values = [2, 1.8, 1.2, 1.9, 1.1, 1.9]\n",
    "    \"\"\",\n",
    "    llm_config=llm_config2\n",
    ")\n",
    "\n",
    "img_critic_assistant = autogen.AssistantAgent(\n",
    "    name=\"img_critic_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\"\"\"You are an advertising image critic AI assistant. \n",
    "Your task is to critique the 'output.json' from 'img_blend_assistant'.\n",
    "Recommend 'img_blend_assistant' for better advertising by comparing it to image 1, which is a good advertisement on the above metrics.\n",
    "Rate the the blended image from 1 to 10 considering the brightness, position and layer.\n",
    "Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config=llm_config2\n",
    ")\n",
    "\n",
    "# The user proxy agent is used for interacting with the assistant agent and executes tool calls.\n",
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    system_message=\"Executor. Execute the functions recommended by the assistants.\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "# Register functions for execution\n",
    "img_blend_assistant.register_for_llm(name=\"blend_images\", description=\"Image blender\")(blend_images)\n",
    "img_blend_assistant.register_for_llm(name=\"describe_images_from_json\", description=\"describe_images_from_json\")(describe_images_from_json)\n",
    "img_critic_assistant.register_for_llm(name=\"read_img\", description=\"Image reader\")(read_img) \n",
    "user_proxy.register_for_execution(name=\"blend_images\")(blend_images)\n",
    "user_proxy.register_for_execution(name=\"read_img\")(read_img)\n",
    "user_proxy.register_for_execution(name=\"describe_images_from_json\")(describe_images_from_json)\n",
    "\n",
    "\n",
    "# Create group chat\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, img_blend_assistant, img_critic_assistant],\n",
    "    messages=[],  # The initial messages in the chat\n",
    "    max_round=5  # Maximum rounds of conversation\n",
    ")\n",
    "\n",
    "# Create group chat manager\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "blend the images '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/notebooks/img_path.json' \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_blend_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimg_blend_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_W2UmEuh7kiJrP2I3WhQ3C5n8): describe_images_from_json *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"json_file_path\": \"/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/notebooks/img_path.json\"\n",
      "}\n",
      "\u001b[32m******************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION describe_images_from_json...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_W2UmEuh7kiJrP2I3WhQ3C5n8) *****\u001b[0m\n",
      "{\"larger_image_description\": \"The image shows two black boxes labeled \\\"OFFBLAK Generation T Taster Pack\\\" with holographic numbers 1 and 2 on them. There are also two colorful packets of OFFBLAK tea labeled \\\"Brighten Up\\\" and \\\"Future is Pink.\\\" Additionally, it's captioned with the text \\\"Subscribe to Taster Pack Combo today and get 50% OFF!\\\" The image also includes sunglasses, a cup of tea with an OFFBLAK tag, and a black textured item, possibly a wallet or a notebook. There is also a leather jacket partially visible on the left side.\", \"small_images_descriptions\": [{\"image_path\": \"/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/cta.jpg\", \"description\": \"The image contains the text \\\"SHOP NOW\\\" written in bold, black capital letters on a white background.\"}, {\"image_path\": \"/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/discover.png\", \"description\": \"The image contains text that reads: \\\"Discover 12 unique tea flavours delivered to your door.\\\"\"}, {\"image_path\": \"/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_3.png\", \"description\": \"The image contains the text \\\"Enjoy tea delivered to your door\\\".\"}, {\"image_path\": \"/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_animation_1.png\", \"description\": \"The image depicts a white silhouette of a hand with an extended index finger, appearing to press or touch an icon or button. It is commonly used to represent the action of tapping or clicking.\"}, {\"image_path\": \"/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_instruction_1.png\", \"description\": \"The image contains text that reads: \\\"TAP To get Letterbox Delivery of Tea\\\".\"}, {\"image_path\": \"/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/landing_endframe.jpg\", \"description\": \"The image contains a logo for \\\"OFFBLAK,\\\" with the text \\\"Generation T\\\" written underneath it. The logo and text are in black and appear on a white background.\"}]}\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_blend_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimg_blend_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the description of the images, we can conclude the following:\n",
      "\n",
      "1. \"SHOP NOW\": This CTA (call-to-action) button is crucial and should be easily seen. So, I recommend placing it at the \"center center\" so that it catches the viewer's eye immediately. We will use the highest layer (1) and increased brightness (1.9) for this image to make it pop.\n",
      "\n",
      "2. \"Discover 12 unique tea flavours delivered to your door.\": This text provides a value proposition for the product, so it's important to make it visible. Let's put it at the 'top center'. This one can be on the second layer (2) and a brightness value of slightly less than the maximum (1.8).\n",
      "\n",
      "3. \"Enjoy tea delivered to your door\": This is another value proposition. Placing it at the 'top right' might balance the composition. As it's less impactful as the second image, let's put it at the third layer (3) and decreases its brightness by 0.6 points (1.2).\n",
      "\n",
      "4. Hand Silhouette: This image represents interactivity and can be overlaid anywhere. Placing it at 'bottom left' might complement the composition without covering important parts. It can be placed at the fourth layer (4) and should be slightly brighter (1.7) to highlight the possible interaction.\n",
      "\n",
      "5. \"TAP To get Letterbox Delivery of Tea\": This instruction should accompany the hand silhouette. Placing it also at 'bottom left' complement the previous image's position. It should have the same brightness as the silhouette and the same layer (4).\n",
      "\n",
      "6. \"OFFBLAK,\" with \"Generation T\": This is the logo and it has great importance. To balance the composition, this image can go at the 'bottom center'. Put it at the highest layer (1) and maximum brightness (2) to make it really stand out.\n",
      "\n",
      "Let's blend the images using these settings.\n",
      "\u001b[32m***** Suggested tool call (call_W2UmEuh7kiJrP2I3WhQ3C5n8): blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"position_names\": [\"center center\", \"top center\", \"top right\", \"bottom left\", \"bottom left\", \"bottom center\"],\n",
      "\"brightness_values\": [1.9, 1.8, 1.2, 1.7, 1.7, 2],\n",
      "\"layers\": [1, 2, 3, 4, 4, 1]\n",
      "}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "Number of small images: 6\n",
      "Number of positions: 6\n",
      "Number of brightness values: 6\n",
      "Number of layers: 6\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_W2UmEuh7kiJrP2I3WhQ3C5n8) *****\u001b[0m\n",
      "./output/output.json\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(manager, message=\"blend the images '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/notebooks/img_path.json' \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
