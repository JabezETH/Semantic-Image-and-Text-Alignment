{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access your OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def json_to_image(image_path: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Converts an image file to a PIL Image object.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image.Image: Image as a PIL Image object.\n",
    "    \"\"\"\n",
    "    return Image.open(image_path)\n",
    "\n",
    "def read_img(json_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads an image path from a JSON file, converts both a constant image and the image from JSON to JPEG, and sends them to the OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    - json_path (str): Path to the JSON file containing the image path.\n",
    "    - openai_api_key (str): OpenAI API key for authentication.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Response from the OpenAI API.\n",
    "    \"\"\"\n",
    "    # Constant image path\n",
    "    constant_image_path = \"/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/_preview.png\"\n",
    "\n",
    "    # Load JSON data from file\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    \n",
    "    # Extract the image path from the JSON data\n",
    "    image_path = json_data['output_path']\n",
    "\n",
    "    # Convert image files to PIL Images\n",
    "    constant_image = json_to_image(constant_image_path).convert(\"RGB\")\n",
    "    image = json_to_image(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Convert images to base64 strings in JPEG format\n",
    "    buffered1 = BytesIO()\n",
    "    constant_image.save(buffered1, format=\"JPEG\")\n",
    "    base64_constant_image = base64.b64encode(buffered1.getvalue()).decode('utf-8')\n",
    "\n",
    "    buffered2 = BytesIO()\n",
    "    image.save(buffered2, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(buffered2.getvalue()).decode('utf-8')\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"compare the two image object positions. and suggest what to change in image 2 to make it like image 1\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_constant_image}\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    response_data = response.json()\n",
    "    return response_data['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make the second image more like the first image in terms of object positioning and layout, you can make the following adjustments:\\n\\n1. **Title Placement:**\\n   - Move the \"OFFBLAK Generation T\" title to the top of the image, similar to the first image.\\n\\n2. **Text Layout:**\\n   - Place the marketing text (\"Discover 12...\") in the middle of the image, similar to how the \"Subscribe to...\" text is centered in the first image.\\n\\n3. **Product Presentation:**\\n   - Display a broader selection of products and place them with some spacing around the middle of the image (like the tea packs and boxes in the first image).\\n\\n4. **Call-to-Action Button:**\\n   - Add a clear \"Shop Now\" button at the bottom of the image, as done in the first image.\\n\\n5. **Supplementary Objects:**\\n   - Add some lifestyle items around the products (e.g., a cup of tea, a pair of sunglasses) to create a similar vibe to the first image.\\n\\n6. **Hand/Icon Element:**\\n   - Place an icon or an element to guide the viewerâ€™s attention to the call to action or important products (similar to the pointing hand in the first image).\\n   \\nHere is a suggested positioning guide to follow: \\n\\n- **Top Section (Brand Name):**\\n  - \"OFFBLAK Generation T\" \\n\\n- **Middle Section (Main Message and Products):**\\n  - Main'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = \"./output/output.json\"\n",
    "read_img(json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_position_from_constant(position_name, larger_image, smaller_image):\n",
    "    \"\"\"\n",
    "    Get the (x, y) coordinates for a given position name based on the size of the larger and smaller images.\n",
    "\n",
    "    Parameters:\n",
    "    - position_name (str): Name of the position (e.g., \"top right\").\n",
    "    - larger_image (PIL.Image.Image): The larger image.\n",
    "    - smaller_image (PIL.Image.Image): The smaller image.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (x, y) coordinates for the given position.\n",
    "    \"\"\"\n",
    "    larger_w, larger_h = larger_image.size\n",
    "    smaller_w, smaller_h = smaller_image.size\n",
    "\n",
    "    positions = {\n",
    "        \"top left\": (0, 0),\n",
    "        \"top center\": ((larger_w - smaller_w) // 2, 0),\n",
    "        \"top right\": (larger_w - smaller_w, 0),\n",
    "        \"center left\": (0, (larger_h - smaller_h) // 2),\n",
    "        \"center center\": ((larger_w - smaller_w) // 2, (larger_h - smaller_h) // 2),\n",
    "        \"center right\": (larger_w - smaller_w, (larger_h - smaller_h) // 2),\n",
    "        \"bottom left\": (0, larger_h - smaller_h),\n",
    "        \"bottom center\": ((larger_w - smaller_w) // 2, larger_h - smaller_h),\n",
    "        \"bottom right\": (larger_w - smaller_w, larger_h - smaller_h),\n",
    "    }\n",
    "\n",
    "    return positions.get(position_name, (0, 0))\n",
    "\n",
    "def blend_images(position_names: list, alpha: float = 0.5, output_dir: str = './output') -> str:\n",
    "    \"\"\"\n",
    "    Blends multiple small images by placing them on top of a larger image at specified positions without overlapping and saves the blended image.\n",
    "\n",
    "    Parameters:\n",
    "    - position_names (list of str): List of position names for each small image, e.g., [\"top left\", \"top right\", \"center center\"].\n",
    "    - alpha (float): Blending factor for transparency (0.0 to 1.0). Default is 0.5.\n",
    "    - output_dir (str): Directory where the blended image will be saved. Default is './output'.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the JSON file containing the output image information.\n",
    "    \"\"\"\n",
    "    # Define paths to small images (example paths)\n",
    "    small_images_paths = [\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/cta.jpg',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/discover.png',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_3.png',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_animation_1.png',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_instruction_1.png',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/landing_endframe.jpg'\n",
    "    ]\n",
    "\n",
    "    if len(small_images_paths) != len(position_names):\n",
    "        raise ValueError(\"The number of small images must match the number of position names.\")\n",
    "\n",
    "    # Load larger image from file path (example path)\n",
    "    larger_image_path = '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_1.jpg'\n",
    "    larger_image = Image.open(larger_image_path)\n",
    "    larger_image = larger_image.convert(\"RGBA\")  # Ensure the larger image has an alpha channel\n",
    "\n",
    "    # Create a blank image with the same size as the larger image\n",
    "    blended_image = Image.new(\"RGBA\", larger_image.size)\n",
    "\n",
    "    # Paste the larger image onto the blank image\n",
    "    blended_image.paste(larger_image, (0, 0))\n",
    "\n",
    "    # Blend each small image at its respective position\n",
    "    for i, small_image_path in enumerate(small_images_paths):\n",
    "        # Load small image from file path\n",
    "        smaller_image = Image.open(small_image_path)\n",
    "        smaller_image = smaller_image.convert(\"RGBA\")  # Ensure the smaller image has an alpha channel\n",
    "\n",
    "        # Resize smaller image if necessary\n",
    "        smaller_w, smaller_h = smaller_image.size\n",
    "        larger_w, larger_h = larger_image.size\n",
    "        if smaller_h > larger_h or smaller_w > larger_w:\n",
    "            aspect_ratio = smaller_w / smaller_h\n",
    "            if smaller_h > larger_h:\n",
    "                smaller_h = larger_h\n",
    "                smaller_w = int(smaller_h * aspect_ratio)\n",
    "            if smaller_w > larger_w:\n",
    "                smaller_w = larger_w\n",
    "                smaller_h = int(smaller_w / aspect_ratio)\n",
    "            smaller_image = smaller_image.resize((smaller_w, smaller_h))\n",
    "\n",
    "        # Make the smaller image semi-transparent\n",
    "        enhancer = ImageEnhance.Brightness(smaller_image)\n",
    "        smaller_image = enhancer.enhance(alpha)\n",
    "\n",
    "        # Get position for the smaller image\n",
    "        x_offset, y_offset = get_position_from_constant(position_names[i], larger_image, smaller_image)\n",
    "\n",
    "        # Paste the smaller image onto the blended image at the calculated position\n",
    "        blended_image.paste(smaller_image, (x_offset, y_offset), smaller_image)\n",
    "\n",
    "    # Save blended image to output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'blended_image.png')\n",
    "    blended_image.save(output_file)\n",
    "\n",
    "    # Create JSON response\n",
    "    json_data = {\n",
    "        'output_path': output_file,\n",
    "        'positions': position_names,\n",
    "        'alpha': alpha\n",
    "    }\n",
    "\n",
    "    # Write JSON to file\n",
    "    json_output_file = os.path.join(output_dir, 'output.json')\n",
    "    with open(json_output_file, 'w') as f:\n",
    "        json.dump(json_data, f, indent=4)\n",
    "\n",
    "    return json_output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/output.json'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions_list =  [\"bottom center\", \"top right\", \"top left\",\"center center\", \"bottom left\", \"top centert\"]\n",
    "\n",
    "# Call the blend_images function\n",
    "blend_images(positions_list, alpha=0.5, output_dir='./output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(positions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config2 = {\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": openai_api_key}]}\n",
    "code_execution_config = {\"use_docker\": False}\n",
    "\n",
    "# Initialize the assistant agent with the given configurations\n",
    "config_list = [\n",
    "    {\"model\": \"gpt-4\", \"api_key\": openai_api_key, \"api_type\": \"openai\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config={\n",
    "        \"temperature\": 0,\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"functions\": [\n",
    "             {\n",
    "                        \"name\": \"blend_images\",\n",
    "                        \"description\": \"use this function to blend the images\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"position_names\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"This is where you will position the blending\"\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"positions_str\"]\n",
    "                        }\n",
    "                        },\n",
    "             {\n",
    "                        \"name\": \"read_img\",\n",
    "                        \"description\": \"use this to read the image blended from blended image\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"output_json\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"description\": \"This is the blended image\"\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"position\"]\n",
    "                        }\n",
    "                        }\n",
    "                        ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "# Let's first define the assistant agent that suggests tool calls.\n",
    "img_blend_assistant = autogen.AssistantAgent(\n",
    "    name=\"image_blending_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\n",
    "    \n",
    "\"\"\"You are a helpful AI assistant. \n",
    "The main problems you will be solving include:\n",
    "- suggest diffrent \"positions\" to make a good advertising based on the feedback from 'img_critic_assistan'\n",
    "    use this as example: \"List of position names for each small image, e.g., [\"top left\", \"top right\", \"center center\", \"bottom left\", \"bottom center\", \"bottom right\"]\"\n",
    "    make sure you are giving only 6 positions\n",
    "    make sure that the images will not overlap\n",
    "- This are the discription of the pictures:\n",
    "    1. the first position is for a picture that have a text saying 'shop now '\n",
    "    2. the second position is for a picture that have a text saying 'Discover 12 unique tea flavours delivered to your door'\n",
    "    3. the third position is for a picture that have a text saying 'Enjoy tea delivered to your home'\n",
    "    4. the fourth position is for a picture that shows a hand pointing\n",
    "    5. the fifth position is for a picture that have a text saying 'tap to get letter box delivery of tea'\n",
    "    6. the sixth position is for a picture that have a text saying 'off black generation picture'\n",
    "\n",
    "- your task:\n",
    "    - considering the above discription for each picture find a way to position each picture to give a good advertaizing \n",
    "    based on the recomendation you got from 'img_critic_assistant'\n",
    "    - 'TERMINATE' when the image you blend looks like the feedback.\n",
    "    \"\"\"\n",
    "    ,\n",
    "    llm_config=llm_config,\n",
    "    function_map={\n",
    "        \"blend_images\": blend_images,\n",
    "        \"read_img\": read_img\n",
    "    }\n",
    "        \n",
    ")\n",
    "\n",
    "img_critic_assistant = autogen.AssistantAgent(\n",
    "    name=\"image_blending_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\"You are a advertizing image critic AI assistant. \"\n",
    "    \"\"\"You task is to critic the 'output.json' from 'img_blend_assistant'\n",
    "    critic only the following metrics: \n",
    "  \n",
    "                            - the first position is for a picture that have a text saying 'shop now '\n",
    "                            - the second position is for a picture that have a text saying 'Discover 12 unique tea flavours delivered to your door'\n",
    "                            - the third position is for a picture that have a text saying 'Enjoy tea delivered to your home'\n",
    "                            - the fourth position is for a picture that shows a hand pointing\n",
    "                            - the fifth position is for a picture that have a text saying 'tap to get letter box delivery of tea'\n",
    "                            - the sixth position is for a picture that have a text saying 'off black generation picture'\n",
    "    recommend 'img_blend_assistant' for a better advertising by comparing to image 1 which is a good advertisement on the above metrics.\n",
    "    \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    function_map={\n",
    "        \"blend_images\": blend_images,\n",
    "        \"read_img\": read_img\n",
    "    }\n",
    "        \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The user proxy agent is used for interacting with the assistant agent\n",
    "# and executes tool calls.\n",
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    system_message=\"Executor. Execute the functions recommended by the assistants.\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False\n",
    "    # # is_termination_msg=lambda x: \"content\" in x and x[\"content\"] is not None and x[\"content\"].rstrip().endswith(\"TERMINATE\"),\n",
    "    # # code_execution_config={\"work_dir\": \"planning\"},\n",
    "    # function_map={\"blend_images\": blend_images},\n",
    "    # code_execution_config=False\n",
    ")\n",
    "\n",
    "def state_transition(last_speaker, groupchat):\n",
    "    messages = groupchat.messages\n",
    "    if last_speaker is user_proxy:\n",
    "        # If user_proxy was last, we decide whether to start blending or critiquing\n",
    "        return img_blend_assistant\n",
    "\n",
    "    elif last_speaker is img_blend_assistant:\n",
    "        # After img_blend_assistant, we need user_proxy to execute some code\n",
    "        # Therefore, the next state should be user_proxy\n",
    "        return img_critic_assistant\n",
    "\n",
    "    elif last_speaker is img_critic_assistant:\n",
    "        # After img_critic_assistant, we need user_proxy to execute some code\n",
    "        # Therefore, the next state should be user_proxy\n",
    "        return img_blend_assistant\n",
    "\n",
    "    # elif last_speaker is user_proxy and 'img_blend_assistant' in messages[-1][\"content\"]:\n",
    "    #     # If user_proxy just executed code for img_blend_assistant, next is img_critic_assistant\n",
    "    #     return img_critic_assistant\n",
    "\n",
    "    # elif last_speaker is user_proxy and 'img_critic_assistant' in messages[-1][\"content\"]:\n",
    "    #     # If user_proxy just executed code for img_critic_assistant, next is img_blend_assistant\n",
    "    #     return img_blend_assistant\n",
    "\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, img_blend_assistant, img_critic_assistant],\n",
    " \n",
    "    messages=[],  # The initial messages in the chat\n",
    "    max_round=5,  # Maximum rounds of conversation\n",
    "    speaker_selection_method=state_transition,\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mimage_blending_assistant\u001b[0m (to image_blending_assistant):\n",
      "\n",
      "blend the images at diffrent positions\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to image_blending_assistant):\n",
      "\n",
      "\u001b[32m***** Suggested function call: blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"position_names\": \"first, second, third, fourth, fifth, sixth\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to image_blending_assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling function (blend_images) *****\u001b[0m\n",
      "Error: The number of small images must match the number of position names.\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to image_blending_assistant):\n",
      "\n",
      "It seems there was an error because the number of images does not match the number of position names. Let's try again by specifying the correct number of images.\n",
      "\u001b[32m***** Suggested function call: blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"position_names\": \"first, second, third, fourth, fifth\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to image_blending_assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling function (blend_images) *****\u001b[0m\n",
      "Error: The number of small images must match the number of position names.\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to image_blending_assistant):\n",
      "\n",
      "I apologize for the confusion. It seems there is still an error due to the mismatch between the number of images and the position names. Let's try again with the correct number of images.\n",
      "\u001b[32m***** Suggested function call: blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"position_names\": \"first, second, third\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to image_blending_assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling function (blend_images) *****\u001b[0m\n",
      "Error: The number of small images must match the number of position names.\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to image_blending_assistant):\n",
      "\n",
      "I apologize for the confusion. It seems there is still an error due to the mismatch between the number of images and the position names. Let's try again with the correct number of images.\n",
      "\u001b[32m***** Suggested function call: blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"position_names\": \"first, second\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to image_blending_assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling function (blend_images) *****\u001b[0m\n",
      "Error: The number of small images must match the number of position names.\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mimg_blend_assistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m   \u001b[49m\u001b[43mimg_critic_assistant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblend the images at diffrent positions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:1018\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m   1020\u001b[0m     summary_method,\n\u001b[1;32m   1021\u001b[0m     summary_args,\n\u001b[1;32m   1022\u001b[0m     recipient,\n\u001b[1;32m   1023\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:655\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    653\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 655\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m     )\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:820\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    818\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:655\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    653\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 655\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m     )\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:820\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    818\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: ConversableAgent.send at line 655 (6 times), ConversableAgent.receive at line 820 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:820\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    818\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:655\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    653\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 655\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m     )\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:818\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:1972\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1970\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1972\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   1974\u001b[0m         log_event(\n\u001b[1;32m   1975\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1976\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1980\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   1981\u001b[0m         )\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:1340\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1339\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1340\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/agentchat/conversable_agent.py:1359\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1359\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/oai/client.py:722\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[0;32m--> 722\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    724\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/autogen/oai/client.py:320\u001b[0m, in \u001b[0;36mOpenAIClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    318\u001b[0m     params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    319\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/openai/resources/chat/completions.py:643\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    642\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/openai/_base_client.py:1266\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1263\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1264\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1265\u001b[0m     )\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/openai/_base_client.py:942\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    941\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/openai/_base_client.py:978\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    975\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 978\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    984\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/week_12_updated/Semantic-Image-and-Text-Alignment/env/lib/python3.8/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1255\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1253\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1254\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "message = user_proxy.initiate_chat(\n",
    "    manager, message=\"blend the images at diffrent positions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Response from calling function (blend_images)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chat \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mchat_history\n\u001b[0;32m----> 2\u001b[0m \u001b[43mchat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResponse from calling function (blend_images)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Response from calling function (blend_images)'"
     ]
    }
   ],
   "source": [
    "chat = message.chat_history\n",
    "chat[-1]['Response from calling function (blend_images)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'blend the images at diffrent positions', 'role': 'assistant'}, {'content': '', 'function_call': {'arguments': '{\\n  \"position_names\": [\"top left\", \"top right\", \"center center\", \"bottom left\", \"bottom center\", \"bottom right\"]\\n}', 'name': 'blend_images'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': './output/output.json', 'name': 'blend_images', 'role': 'function'}, {'content': \"The images have been blended successfully. Now, let's read the blended image to provide a critique.\", 'function_call': {'arguments': '{}', 'name': 'read_img'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': \"Error: read_img() missing 1 required positional argument: 'json_path'\", 'name': 'read_img', 'role': 'function'}, {'content': \"Apologies for the confusion. Let's try reading the image again.\", 'function_call': {'arguments': '{\\n  \"json_path\": \"./output/output.json\"\\n}', 'name': 'read_img'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': 'To make the layout of Image 2 similar to Image 1, you can follow these suggestions based on the object positions and overall structure comparison:\\n\\n1. **Brand Logo Position:**\\n   - **Image 1:** The logo is positioned at the top center.\\n   - **Image 2:** The logo is positioned at the bottom center.\\n   - **Change:** Move the \"OFFBLAK Generation T\" logo to the top center in Image 2.\\n\\n2. **Text Content:**\\n   - **Image 1:** The advertisement text \"Subscribe to Taster Pack Combo today and get 50% OFF!\" is placed directly below the logo.\\n   - **Image 2:** The text \"Enjoy tea delivered to your door\" is placed in the middle.\\n   - **Change:** Move the text \"Enjoy tea delivered to your door\" just below the logo after you move it to the top center.\\n\\n3. **Call to Action Button:**\\n   - **Image 1:** The \"Shop Now\" button is near the bottom center.\\n   - **Image 2:** The \"Shop Now\" button is at the top left.\\n   - **Change:** Move the \"Shop Now\" button to the bottom center, similar to its position in Image 1.\\n\\n4. **Product Display:**\\n   - **Image 1:** The product displays (tea packets and boxes) are spread out below the text.\\n   - **Image 2:** A teapot pouring into', 'name': 'read_img', 'role': 'function'}, {'content': \"Based on the critique, let's modify the positions of the images as follows:\\n\\n1. Move the 'off black generation picture' to the 'top center'.\\n2. Move the 'Enjoy tea delivered to your door' picture just below the 'off black generation picture'.\\n3. Move the 'Shop Now' picture to the 'bottom center'.\\n4. Move the 'Discover 12 unique tea flavours delivered to your door' picture to the 'center left'.\\n5. Move the 'tap to get letter box delivery of tea' picture to the 'center right'.\\n6. Move the 'hand pointing' picture to the 'bottom right'.\\n\\nLet's blend the images again with these new positions.\", 'function_call': {'arguments': '{\\n  \"position_names\": [\"top center\", \"center top\", \"bottom center\", \"center left\", \"center right\", \"bottom right\"]\\n}', 'name': 'blend_images'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': './output/output.json', 'name': 'blend_images', 'role': 'function'}, {'content': \"The images have been blended successfully with the new positions. Now, let's read the blended image to provide a critique.\", 'function_call': {'arguments': '{\\n  \"json_path\": \"./output/output.json\"\\n}', 'name': 'read_img'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': 'To make the second image more like the first image, you need to make several adjustments to the object positions and overall layout. Here\\'s a breakdown of the changes:\\n\\n1. **Text Positioning:**\\n   - Move the \"SHOP NOW\" button from the top to the bottom of the image.\\n   - Move the \"Enjoy tea delivered to your door\" text to the middle of the image, similar to where the text \"Subscribe to Taster Pack Combo today and get 50% OFF!\" is positioned in the first image. \\n\\n2. **Object Placement:**\\n   - Reposition the teapot and glass in a way that they align similarly to the tea packs and boxes in the first image. For instance:\\n     - The teapot pouring tea should be positioned to the left side.\\n     - Place the glass closer to the center or slightly to the right. You might want to include additional elements like the left image, such as tea bags or other related items.\\n\\n3. **Practical Changes:**\\n   - Ensure that the elements in the second image do not obstruct the text as much. Rearrange them so that the text appears more clearly, similar to the unobstructed text in the first image.\\n\\n4. **Brand Logo Placement:**\\n   - The OFFBLAK logo is at the bottom in the second image. Move it to the top, imitating its placement in the first image.\\n\\n5. **Hand Graphic Element:**\\n   - Include a hand graphic similar to', 'name': 'read_img', 'role': 'function'}, {'content': \"Based on the critique, let's modify the positions of the images as follows:\\n\\n1. Move the 'off black generation picture' to the 'top center'.\\n2. Move the 'Enjoy tea delivered to your door' picture to the 'center center'.\\n3. Move the 'Shop Now' picture to the 'bottom center'.\\n4. Move the 'Discover 12 unique tea flavours delivered to your door' picture to the 'center left'.\\n5. Move the 'tap to get letter box delivery of tea' picture to the 'center right'.\\n6. Move the 'hand pointing' picture to the 'bottom right'.\\n\\nLet's blend the images again with these new positions.\", 'function_call': {'arguments': '{\\n  \"position_names\": [\"top center\", \"center center\", \"bottom center\", \"center left\", \"center right\", \"bottom right\"]\\n}', 'name': 'blend_images'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': './output/output.json', 'name': 'blend_images', 'role': 'function'}, {'content': \"The images have been blended successfully with the new positions. Now, let's read the blended image to provide a critique.\", 'function_call': {'arguments': '{\\n  \"json_path\": \"./output/output.json\"\\n}', 'name': 'read_img'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': 'To make the second image more similar to the first image, several adjustments are necessary in terms of object positions and text placement. Here\\'s a detailed comparison and suggestions for modification:\\n\\n### 1. Text Placement and Content:\\n- **Top Section Text:**\\n  - **Image 1:** The top section has \"OFFBLAK GENERATION T\" centered at the top, followed by \"Subscribe to Taster Pack Combo today and get 50% OFF!\" below it.\\n  - **Image 2:** The top section text \"SHOP NOW\" is at the top center, and below it, the text \"Discover 12 tea flavours delivered to your door\" and \"To get Letterbox Delivery of Tea\" are placed on the pouring kettle and glass area.\\n  - **Modification:** Move the text \"OFFBLAK GENERATION T\" to the top center, below that add \"Subscribe to Taster Pack Combo today and get 50% OFF!\" in a clear, centered format.\\n\\n### 2. Product and Object Arrangement:\\n- **Product Boxes and Packs:**\\n  - **Image 1:** Two black product boxes (numbered 1 and 2) and multiple small packs arranged on the right side in a visually appealing manner.\\n  - **Image 2:** Focuses on a pouring kettle and a glass with tea, which doesn\\'t match the arrangement in Image 1.\\n  - **Modification:** Replace the kettle and glass with the two black product boxes (similar to', 'name': 'read_img', 'role': 'function'}, {'content': \"Based on the critique, let's modify the positions of the images as follows:\\n\\n1. Move the 'off black generation picture' to the 'top center'.\\n2. Move the 'Enjoy tea delivered to your door' picture to the 'center center'.\\n3. Move the 'Shop Now' picture to the 'bottom center'.\\n4. Move the 'Discover 12 unique tea flavours delivered to your door' picture to the 'center left'.\\n5. Move the 'tap to get letter box delivery of tea' picture to the 'center right'.\\n6. Move the 'hand pointing' picture to the 'bottom right'.\\n\\nLet's blend the images again with these new positions.\", 'function_call': {'arguments': '{\\n  \"position_names\": [\"top center\", \"center center\", \"bottom center\", \"center left\", \"center right\", \"bottom right\"]\\n}', 'name': 'blend_images'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': './output/output.json', 'name': 'blend_images', 'role': 'function'}, {'content': \"The images have been blended successfully with the new positions. Now, let's read the blended image to provide a critique.\", 'function_call': {'arguments': '{\\n  \"json_path\": \"./output/output.json\"\\n}', 'name': 'read_img'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': 'To make the objects in Image 2 more like those in Image 1, you can follow these steps:\\n\\n1. **Font and Text Placement:**\\n   - **Title and Subtitle Font:** Use a similar black, bold, sans-serif font for the main text elements. Place the main promotional text (\"Subscribe to Taster Pack Combo today and get 50% OFF!\") centered and prominently, as in Image 1.\\n   - **Bottom Text:** Place the \"Enjoy tea delivered to your door\" text in the middle part of the image, just above the OFFBLAK logo, making sure it\\'s central and clear.\\n\\n2. **Object Arrangement:**\\n   - **Central Object:** Remove the kettle pouring tea action. Instead, place the glass with the tea bag centered in the image, similarly to the central arrangement of the tea packs in Image 1.\\n   - **Complementary Items:** Add similar complementary items around the central object. You could use tea packs, accessories like sunglasses, or small objects like tea spoons, scattered casually around the glass.\\n\\n3. **Logo and Branding:**\\n   - Place the OFFBLAK logo at the bottom, centered and clearly visible, similar to Image 1.\\n   - Ensure the \"SHOP NOW\" CTA button is placed at the bottom, above the OFFBLAK logo, centered and easily clickable.\\n\\n4. **Background:**\\n   - Use a clean, neutral background. A white or slightly off-white background will make the', 'name': 'read_img', 'role': 'function'}, {'content': \"Based on the critique, let's modify the positions of the images as follows:\\n\\n1. Move the 'off black generation picture' to the 'top center'.\\n2. Move the 'Enjoy tea delivered to your door' picture to the 'center center'.\\n3. Move the 'Shop Now' picture to the 'bottom center'.\\n4. Move the 'Discover 12 unique tea flavours delivered to your door' picture to the 'center left'.\\n5. Move the 'tap to get letter box delivery of tea' picture to the 'center right'.\\n6. Move the 'hand pointing' picture to the 'bottom right'.\\n\\nLet's blend the images again with these new positions.\", 'function_call': {'arguments': '{\\n  \"position_names\": [\"top center\", \"center center\", \"bottom center\", \"center left\", \"center right\", \"bottom right\"]\\n}', 'name': 'blend_images'}, 'name': 'image_blending_assistant', 'role': 'assistant'}], summary=\"Based on the critique, let's modify the positions of the images as follows:\\n\\n1. Move the 'off black generation picture' to the 'top center'.\\n2. Move the 'Enjoy tea delivered to your door' picture to the 'center center'.\\n3. Move the 'Shop Now' picture to the 'bottom center'.\\n4. Move the 'Discover 12 unique tea flavours delivered to your door' picture to the 'center left'.\\n5. Move the 'tap to get letter box delivery of tea' picture to the 'center right'.\\n6. Move the 'hand pointing' picture to the 'bottom right'.\\n\\nLet's blend the images again with these new positions.\", cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "# Define the assistant agent that suggests tool calls.\n",
    "img_blend_assistant = autogen.AssistantAgent(\n",
    "    name=\"img_blend_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\"\"\"You are a helpful AI assistant. \n",
    "The main problems you will be solving include:\n",
    "- suggest different \"positions\" to make a good advertising based on the feedback from 'img_critic_assistant'\n",
    "    use this as example: \"List of position names for each small image, e.g., [\"top left\", \"top right\", \"center center\", \"bottom left\", \"bottom center\", \"bottom right\"]\"\n",
    "    make sure you are giving only 6 positions\n",
    "    make sure that the images will not overlap\n",
    "- picture positions: ['shop now','Discover 12 unique tea flavours delivered to your door', 'Enjoy tea delivered to your home', hand pointing, 'tap to get letter box delivery of tea','off black generation picture']\n",
    "- Your task:\n",
    "    - Considering the above descriptions for each picture, find a way to position each picture to give good advertising based on the recommendation you got from 'img_critic_assistant'.\n",
    "    - 'TERMINATE' when the image you blend looks like the feedback.\n",
    "    \"\"\",\n",
    "    llm_config=llm_config2\n",
    ")\n",
    "\n",
    "img_critic_assistant = autogen.AssistantAgent(\n",
    "    name=\"img_critic_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\"\"\"You are an advertising image critic AI assistant. \n",
    "Your task is to critique the 'output.json' from 'img_blend_assistant'.\n",
    "positions: ['shop now','Discover 12 unique tea flavours delivered to your door', 'Enjoy tea delivered to your home', hand pointing, 'tap to get letter box delivery of tea','off black generation picture']\n",
    "Recommend 'img_blend_assistant' for better advertising by comparing it to image 1, which is a good advertisement on the above metrics.\n",
    "Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config=llm_config2\n",
    ")\n",
    "\n",
    "# The user proxy agent is used for interacting with the assistant agent and executes tool calls.\n",
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    system_message=\"Executor. Execute the functions recommended by the assistants.\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "# Register functions for execution\n",
    "img_blend_assistant.register_for_llm(name=\"blend_images\", description=\"Image blender\")(blend_images)\n",
    "img_critic_assistant.register_for_llm(name=\"read_img\", description=\"Image reader\")(read_img) \n",
    "user_proxy.register_for_execution(name=\"blend_images\")(blend_images)\n",
    "user_proxy.register_for_execution(name=\"read_img\")(read_img)\n",
    "\n",
    "# Create group chat\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, img_blend_assistant, img_critic_assistant],\n",
    "    messages=[],  # The initial messages in the chat\n",
    "    max_round=15  # Maximum rounds of conversation\n",
    ")\n",
    "\n",
    "# Create group chat manager\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "blend the images at diffrent positions\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_blend_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimg_blend_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure, let's find suitable positions for the images while considering that they won't overlap:\n",
      "\n",
      "1. 'shop now' - this is usually a call to action, it would usually be bottom center or top right. Let's place it at \"bottom center\".\n",
      "\n",
      "2. 'Discover 12 unique tea flavours delivered to your door' - This is a key selling point, so it should be prominent but not overpowering. I'd suggest placing it at \"top right\".\n",
      "\n",
      "3. 'Enjoy tea delivered to your home' - This is reinforcing the convenience of the service, so we can place it at  \"top left\".\n",
      "\n",
      "4. 'hand pointing' - The hand pointing usually directs attention to something important, so its position would depend on what it's pointing at. If it's pointing at 'shop now' then it could be at 'bottom right'.\n",
      "\n",
      "5. 'tap to get letter box delivery of tea' - It seems an interactive feature, nice to place it at \"center center\" where it would easily grasp attention.\n",
      "\n",
      "6. 'off black generation picture' - This can be the background, it could be placed at \"bottom left\" if it's not too distracting.\n",
      "\n",
      "Let's try this arrangement and then evaluate the output. If the 'img_critic_assistant' gives further recommendations, we can adjust the image positions accordingly. \n",
      "\n",
      "Now, let's call the image blending function with these positions.\n",
      "\u001b[32m***** Suggested tool call (call_3RxebePg7Ozdm06BKzupIifi): blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"position_names\": [\"bottom center\", \"top right\", \"top left\", \"bottom right\", \"center center\", \"bottom left\"]\n",
      "}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_3RxebePg7Ozdm06BKzupIifi) *****\u001b[0m\n",
      "./output/output.json\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_critic_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimg_critic_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_iNHRYaUmcjs5GyrfWpxwQMr5): read_img *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"json_path\": \"./output/output.json\"\n",
      "}\n",
      "\u001b[32m*************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION read_img...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_iNHRYaUmcjs5GyrfWpxwQMr5) *****\u001b[0m\n",
      "To make the objects in Image 2 more like the objects in Image 1, you'll need to focus on the composition and the arrangement of the elements:\n",
      "\n",
      "1. **Logo and Text Placement:**\n",
      "   - In Image 1, the \"OFFBLAK\" logo is at the top center. If you want to make Image 2 similar, move the \"OFFBLAK\" logo to the top center.\n",
      "   - The text \"Generation T\" under the logo in Image 1 can be added directly under the logo in Image 2.\n",
      "\n",
      "2. **Main Promotional Text:**\n",
      "   - In Image 1, the promotional text \"Subscribe to Taster Pack Combo today and get 50% OFF!\" is centered below the logo. Align similar promotional text in Image 2 (like \"Enjoy tea delivered to your door\") directly below the logo at the top.\n",
      "   - Image 1 has a hand icon clicking on one of the boxes. You could add this \"TAP\" text makeshift icon indicating interaction in a similar manner in Image 2.\n",
      "\n",
      "3. **Main Product Display:**\n",
      "   - Image 1 displays the products (tea boxes and tea bags) prominently in the center. Rearrange the teapot and tea glass in Image 2 so they are centered.\n",
      "   - Ensure the main items (teapot, tea glass) are the focal points, similar to how the boxes and bags are in Image 1.\n",
      "\n",
      "4. **Call to Action Button:\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_blend_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimg_blend_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems like the initial arrangement we suggested needs some adjustments. Based on 'img_critic_assistant' feedback:\n",
      "\n",
      "1. 'shop now' - should stay at the \"bottom center\".\n",
      "2. 'Discover 12 unique tea flavours delivered to your door' - should be moved to \"center center\".\n",
      "3. 'Enjoy tea delivered to your home' - should stay at  \"top left\".\n",
      "4. 'hand pointing' - should be moved to \"top center\", pointing at the 'Enjoy tea delivered to your home'.\n",
      "5. 'tap to get letter box delivery of tea' - should be moved to \"bottom right\" where it would easily grasp attention.\n",
      "6. 'off black generation picture' - should stay at \"bottom left\".\n",
      "\n",
      "Let's adjust our images and recall the blend_images function.\n",
      "\u001b[32m***** Suggested tool call (call_CZFXaJpS5JP2sjzsfZ8zVzoG): blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"position_names\": [\"bottom center\", \"center center\", \"top left\", \"top center\", \"bottom right\", \"bottom left\"]\n",
      "}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_CZFXaJpS5JP2sjzsfZ8zVzoG) *****\u001b[0m\n",
      "./output/output.json\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_critic_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimg_critic_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_Ji5ZR9YoRTrjyDWUP24Pu2Yf): read_img *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"json_path\": \"./output/output.json\"\n",
      "}\n",
      "\u001b[32m*************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION read_img...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_Ji5ZR9YoRTrjyDWUP24Pu2Yf) *****\u001b[0m\n",
      "To make the second image more like the first image, you need to consider the following changes:\n",
      "\n",
      "1. **Logo Placement**:\n",
      "   - **Current**: In Image 2, the logo is placed at the bottom.\n",
      "   - **Suggested Change**: Move the logo to the top-center similar to Image 1.\n",
      "\n",
      "2. **Text Positioning**:\n",
      "   - **Current**: Image 2 has text at the top (\"Enjoy tea delivered to your door\"), middle (\"Discover 12 unique tea flavours delivered to your door\") and bottom (\"Enjoy tea delivered to your door\").\n",
      "   - **Suggested Change**: Consolidate the text in the middle with clear, concise messaging, similar to the \"Subscribe to Taster Pack Combo today and get 50% OFF!\" text in Image 1. Use one line of text, centrally placed near the top.\n",
      "\n",
      "3. **Product Display**: \n",
      "   - **Current**: Image 2 shows a kettle and a glass.\n",
      "   - **Suggested Change**: Showcase the products prominently in a similar fashion to Image 1, where multiple products are laid out. Replace the kettle and glass with product packaging.\n",
      "\n",
      "4. **Call-to-Action Button**:\n",
      "   - **Current**: Image 2 does not have a call-to-action button.\n",
      "   - **Suggested Change**: Add a \"Shop Now\" button at the bottom center, like in Image 1.\n",
      "\n",
      "5. **Props and Additional Elements**:\n",
      "   - **Current\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_blend_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimg_blend_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the latest feedback, let's further adjust the image positions:\n",
      "\n",
      "1. 'shop now' - should stay at the \"bottom center\".\n",
      "2. 'Discover 12 unique tea flavours delivered to your door' - should be moved to \"top center\".\n",
      "3. 'Enjoy tea delivered to your home' - should be moved to \"center center\".\n",
      "4. 'hand pointing' - should be moved to \"top right\", pointing at the 'Enjoy tea delivered to your home'.\n",
      "5. 'tap to get letter box delivery of tea' - should stay at \"bottom right\".\n",
      "6. 'off black generation picture' - should stay at \"bottom left\".\n",
      "\n",
      "Let's adjust our images accordingly and blend them again.\n",
      "\u001b[32m***** Suggested tool call (call_HCkdjCwyqNWXL0eQK3U647cr): blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"position_names\": [\"bottom center\", \"top center\", \"center center\", \"top right\", \"bottom right\", \"bottom left\"]\n",
      "}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_HCkdjCwyqNWXL0eQK3U647cr) *****\u001b[0m\n",
      "./output/output.json\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_critic_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimg_critic_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_BpkqUZZpFTv4drWLJ5FbvKnN): read_img *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"json_path\": \"./output/output.json\"\n",
      "}\n",
      "\u001b[32m*************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION read_img...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_BpkqUZZpFTv4drWLJ5FbvKnN) *****\u001b[0m\n",
      "To make the layout of the second image similar to the first image, consider the following changes:\n",
      "\n",
      "1. **Logo and Tagline Position**:\n",
      "   - In Image 1, the OFFBLAK logo and tagline \"GENERATION T\" are positioned at the top center.\n",
      "   - In Image 2, the logo and tagline are positioned at the bottom.\n",
      "   - **Change for Image 2**: Move the OFFBLAK logo and \"GENERATION T\" tagline to the top center of the image.\n",
      "\n",
      "2. **Main Text and Call-to-Action**:\n",
      "   - Image 1 features a central text \"Subscribe to Taster Pack Combo today and get 50% OFF!\" and a \"SHOP NOW\" button beneath it, both centered.\n",
      "   - Image 2 has the text \"Discover 12 unique tea flavours delivered to your door\" at the top and \"Enjoy tea delivered to your door\" text duplicated below the glass.\n",
      "   - **Change for Image 2**: Remove the lower duplicated text \"Enjoy tea delivered to your door\" and move the top text to the center. Add a \"SHOP NOW\" button beneath the central text.\n",
      "\n",
      "3. **Product Display**:\n",
      "   - Image 1 shows multiple product items arrayed around the central text, including tea packs, Taster Pack boxes, sunglasses, and a drink.\n",
      "   - Image 2 shows a teapot pouring tea into a glass, with a single sachet visible.\n",
      "   - **Change\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: img_blend_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimg_blend_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the latest feedback, let's further adjust the image positions:\n",
      "\n",
      "1. 'shop now' - should stay at the \"bottom center\".\n",
      "2. 'Discover 12 unique tea flavours delivered to your door' - should be moved to \"center center\".\n",
      "3. 'Enjoy tea delivered to your home' - should be removed as recommended.\n",
      "4. 'hand pointing' - should be moved to \"top center\", pointing at the 'Discover 12 unique tea flavours delivered to your door'.\n",
      "5. 'tap to get letter box delivery of tea' - should stay at \"bottom right\".\n",
      "6. 'off black generation picture' - should be moved to \"top left\".\n",
      "\n",
      "Let's adjust our images accordingly and blend them again.\n",
      "\u001b[32m***** Suggested tool call (call_p8lpo28CM5q0Ikjs3w3qc5HW): blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"position_names\": [\"bottom center\", \"center center\", \"top center\", \"bottom right\", \"top left\"]\n",
      "}\n",
      "\u001b[32m*****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_p8lpo28CM5q0Ikjs3w3qc5HW) *****\u001b[0m\n",
      "Error: The number of small images must match the number of position names.\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(manager, message=\"blend the images at diffrent positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "\n",
    "# Assistant agent for blending images\n",
    "img_blend_assistant = autogen.AssistantAgent(\n",
    "    name=\"image_blending_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\"\"\"You are a helpful AI assistant. \n",
    "The main problems you will be solving include:\n",
    "- Suggest different \"positions\" to make a good advertisement based on the feedback from 'img_critic_assistant'\n",
    "  Use this as example: \"List of position names for each small image, e.g., [\"top left\", \"top right\", \"center center\", \"bottom left\", \"bottom center\", \"bottom right\"]\"\n",
    "  Make sure you are giving only 6 positions\n",
    "  Make sure that the images will not overlap\n",
    "- This is the description of the pictures:\n",
    "  1. The first position is for a picture that has a text saying 'shop now'\n",
    "  2. The second position is for a picture that has a text saying 'Discover 12 unique tea flavours delivered to your door'\n",
    "  3. The third position is for a picture that has a text saying 'Enjoy tea delivered to your home'\n",
    "  4. The fourth position is for a picture that shows a hand pointing\n",
    "  5. The fifth position is for a picture that has a text saying 'tap to get letter box delivery of tea'\n",
    "  6. The sixth position is for a picture that has a text saying 'off black generation picture'\n",
    "\n",
    "- Your task:\n",
    "  - Considering the above description for each picture, find a way to position each picture to give a good advertisement based on the recommendation you got from 'img_critic_assistant'\n",
    "  - 'TERMINATE' when the image you blend looks like the feedback.\n",
    "\"\"\",\n",
    "    llm_config=llm_config2,\n",
    ")\n",
    "\n",
    "# Assistant agent for criticizing the blended image\n",
    "img_critic_assistant = autogen.AssistantAgent(\n",
    "    name=\"image_critic_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\"\"\"You are an advertising image critic AI assistant. \n",
    "Your task is to criticize the 'output.json' from 'img_blend_assistant' based on the following metrics:\n",
    "- The first position is for a picture that has a text saying 'shop now'\n",
    "- The second position is for a picture that has a text saying 'Discover 12 unique tea flavours delivered to your door'\n",
    "- The third position is for a picture that has a text saying 'Enjoy tea delivered to your home'\n",
    "- The fourth position is for a picture that shows a hand pointing\n",
    "- The fifth position is for a picture that has a text saying 'tap to get letter box delivery of tea'\n",
    "- The sixth position is for a picture that has a text saying 'off black generation picture'\n",
    "Recommend 'img_blend_assistant' for better advertising by comparing to image 1 which is a good advertisement on the above metrics.\n",
    "Return 'TERMINATE' when the task is done.\n",
    "\"\"\",\n",
    "    llm_config=llm_config2,\n",
    ")\n",
    "\n",
    "# Define the termination message check\n",
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "# User proxy agent for executing functions\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    system_message=\"Executor. Execute the functions recommended by the assistants.\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "# Assuming blend_images and read_img are defined elsewhere or imported\n",
    "img_blend_assistant.register_for_llm(name=\"blend_images\", description=\"Image blender\")(blend_images)\n",
    "img_critic_assistant.register_for_llm(name=\"read_img\", description=\"Image reader\")(read_img)\n",
    "user_proxy.register_for_execution(name=\"blend_images\")(blend_images)\n",
    "user_proxy.register_for_execution(name=\"read_img\")(read_img)\n",
    "\n",
    "# Group chat setup\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, img_blend_assistant, img_critic_assistant],\n",
    "    messages=[],  # The initial messages in the chat\n",
    "    max_round=10,  # Maximum rounds of conversation\n",
    ")\n",
    "\n",
    "# Group chat manager setup\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config2\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
