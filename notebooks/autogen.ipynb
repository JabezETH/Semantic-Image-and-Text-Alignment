{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.54 ðŸš€ Python-3.8.10 torch-2.3.1 CPU (aarch64)\n",
      "Setup complete âœ… (16 CPUs, 30.8 GB RAM, 52.0/193.6 GB disk)\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jabez_kassa/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-7-12 Python-3.8.10 torch-2.3.1 CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "def describe_objects_in_images(image_paths):\n",
    "    descriptions = {}\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        # Ensure the image path is valid\n",
    "        if not os.path.isfile(img_path):\n",
    "            descriptions[img_path] = \"Invalid image path.\"\n",
    "            continue\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Perform inference\n",
    "        results = model(img)\n",
    "\n",
    "        # Extract predictions\n",
    "        predictions = results.pred[0]  # Assuming we are only interested in the first image if batch\n",
    "\n",
    "        # Convert predictions to descriptions\n",
    "        objects = []\n",
    "        for *box, conf, cls in predictions:\n",
    "            label = results.names[int(cls)]\n",
    "            objects.append(f\"{label} ({conf:.2f})\")\n",
    "\n",
    "        # Save descriptions\n",
    "        descriptions[img_path] = objects\n",
    "    \n",
    "    return descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "def read_img(json_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads image paths from a JSON file, converts the images to JPEG, and sends them to the OpenAI API for description.\n",
    "\n",
    "    Parameters:\n",
    "    - json_path (str): Path to the JSON file containing the image paths.\n",
    "    - openai_api_key (str): OpenAI API key for authentication.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Response from the OpenAI API.\n",
    "    \"\"\"\n",
    "    # Load JSON data from file\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    \n",
    "    # Extract the image paths from the JSON data\n",
    "    image_paths = json_data['output_paths']\n",
    "    \n",
    "    # Process each image\n",
    "    images_base64 = []\n",
    "    for image_path in image_paths:\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Convert image mode if necessary\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Convert image to base64 string in JPEG format\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        base64_image = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "        images_base64.append(base64_image)\n",
    "    \n",
    "    # Construct the payload with multiple images\n",
    "    messages = []\n",
    "    for idx, base64_image in enumerate(images_base64):\n",
    "        order = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\"]\n",
    "        message_content = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Please describe the {order[idx]} image.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        messages.extend(message_content)\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    \n",
    "    # Check for API response errors\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"OpenAI API error: {response.status_code} {response.text}\")\n",
    "    \n",
    "    response_data = response.json()\n",
    "    \n",
    "    # Check if 'choices' key is present in the response\n",
    "    if 'choices' not in response_data:\n",
    "        raise KeyError(\"'choices' key not found in OpenAI API response\")\n",
    "    \n",
    "    # Extract and return responses\n",
    "    descriptions = [resp['message']['content'] for resp in response_data['choices']]\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "OpenAI API error: 400 {\n  \"error\": {\n    \"message\": \"Invalid type for 'messages[1].content': expected one of a string or array of objects, but got an object instead.\",\n    \"type\": \"invalid_request_error\",\n    \"param\": \"messages[1].content\",\n    \"code\": \"invalid_type\"\n  }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m json_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/notebooks/img_path.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m descriptions \u001b[38;5;241m=\u001b[39m \u001b[43mread_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, description \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(descriptions):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescription for image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 76\u001b[0m, in \u001b[0;36mread_img\u001b[0;34m(json_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Check for API response errors\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI API error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m response_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Check if 'choices' key is present in the response\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: OpenAI API error: 400 {\n  \"error\": {\n    \"message\": \"Invalid type for 'messages[1].content': expected one of a string or array of objects, but got an object instead.\",\n    \"type\": \"invalid_request_error\",\n    \"param\": \"messages[1].content\",\n    \"code\": \"invalid_type\"\n  }\n}"
     ]
    }
   ],
   "source": [
    "json_path='/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/notebooks/img_path.json'\n",
    "descriptions = read_img(json_path)\n",
    "for i, description in enumerate(descriptions):\n",
    "    print(f\"Description for image {i+1}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_images_paths = [\n",
    "'/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/cta.jpg',\n",
    "'/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/discover.png',\n",
    "'/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_3.png',\n",
    "'/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_animation_1.png',\n",
    "'/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_instruction_1.png',\n",
    "'/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/landing_endframe.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/cta.jpg': [],\n",
       " '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/discover.png': ['keyboard (0.36)'],\n",
       " '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_3.png': [],\n",
       " '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_animation_1.png': [],\n",
       " '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_instruction_1.png': [],\n",
       " '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/landing_endframe.jpg': []}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_objects_in_images(small_images_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access your OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def encode_image(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Encodes an image to a base64 string.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input image.\n",
    "\n",
    "    Returns:\n",
    "    - str: Base64-encoded image data.\n",
    "    \"\"\"\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_data = image_file.read()\n",
    "    return base64.b64encode(image_data).decode('utf-8')\n",
    "\n",
    "def json_to_image(image_path: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Converts an image file to a PIL Image object.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "    - PIL.Image.Image: Image as a PIL Image object.\n",
    "    \"\"\"\n",
    "    # Open the image file\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def read_img(json_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads an image path from a JSON file, converts the image to JPEG, and sends it to the OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    - json_path (str): Path to the JSON file containing the image path.\n",
    "    - openai_api_key (str): OpenAI API key for authentication.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Response from the OpenAI API.\n",
    "    \"\"\"\n",
    "    # Load JSON data from file\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    \n",
    "    # Extract the image path from the JSON data\n",
    "    image_path = json_data['output_path']\n",
    "    \n",
    "    # Convert image file to PIL Image\n",
    "    image = json_to_image(image_path)\n",
    "    \n",
    "    # Convert image to base64 string in JPEG format\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    base64_image = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"describe the image\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    response_data =  response.json()\n",
    "    return response_data['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"./output/output.json\"\n",
    "read_img(json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# def blend_images(positions, alpha: float = 0.5, output_dir: str = './output') -> str:\n",
    "#     # Example usage\n",
    "\n",
    "#     small_images_paths = [\n",
    "#     '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/cta.jpg',\n",
    "#     '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/discover.png',\n",
    "#     '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_3.png',\n",
    "#     '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_animation_1.png',\n",
    "#     '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_instruction_1.png',\n",
    "#     '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/landing_endframe.jpg'\n",
    "# ]\n",
    "#     \"\"\"\n",
    "#     Blends multiple small images by placing them on top of a larger image at specified positions without overlapping and saves the blended image.\n",
    "\n",
    "#     Parameters:\n",
    "#     - small_images_paths (list of str): List of paths to the small images.\n",
    "#     - positions (list of tuples): List of (x, y) positions where each small image will be placed on the larger image.\n",
    "#     - alpha (float): Blending factor for transparency (0.0 to 1.0). Default is 0.5.\n",
    "#     - output_dir (str): Directory where the blended image will be saved. Default is './output'.\n",
    "\n",
    "#     Returns:\n",
    "#     - str: Path to the JSON file containing the output image information.\n",
    "#     \"\"\"\n",
    "#     if len(small_images_paths) != len(positions):\n",
    "#         raise ValueError(\"The number of small images must match the number of positions.\")\n",
    "\n",
    "#     larger_image_path = '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_1.jpg'\n",
    "    \n",
    "#     # Load larger image from file path\n",
    "#     larger_image = Image.open(larger_image_path)\n",
    "#     larger_image_np = np.array(larger_image)\n",
    "    \n",
    "#     # Get dimensions of the larger image\n",
    "#     larger_h, larger_w = larger_image_np.shape[:2]\n",
    "\n",
    "#     blended_image_np = larger_image_np.copy()\n",
    "\n",
    "#     for i, small_image_path in enumerate(small_images_paths):\n",
    "#         # Load small image from file path\n",
    "#         smaller_image = Image.open(small_image_path)\n",
    "#         smaller_image_np = np.array(smaller_image)\n",
    "\n",
    "#         # Get dimensions of the smaller image\n",
    "#         smaller_h, smaller_w = smaller_image_np.shape[:2]\n",
    "\n",
    "#         # Resize smaller image if necessary\n",
    "#         if smaller_h > larger_h or smaller_w > larger_w:\n",
    "#             aspect_ratio = smaller_w / smaller_h\n",
    "#             if smaller_h > larger_h:\n",
    "#                 smaller_h = larger_h\n",
    "#                 smaller_w = int(smaller_h * aspect_ratio)\n",
    "#             if smaller_w > larger_w:\n",
    "#                 smaller_w = larger_w\n",
    "#                 smaller_h = int(smaller_w / aspect_ratio)\n",
    "#             smaller_image = smaller_image.resize((smaller_w, smaller_h))\n",
    "#             smaller_image_np = np.array(smaller_image)\n",
    "\n",
    "#         x_offset, y_offset = positions[i]\n",
    "\n",
    "#         # Ensure the small image does not go out of bounds\n",
    "#         if x_offset + smaller_w > larger_w or y_offset + smaller_h > larger_h:\n",
    "#             raise ValueError(f\"Small image at position {positions[i]} exceeds the bounds of the larger image.\")\n",
    "\n",
    "#         # Blend images\n",
    "#         if smaller_image_np.ndim == 2:  # Grayscale image\n",
    "#             smaller_image_np = np.stack([smaller_image_np] * 3, axis=-1)  # Convert to 3-channel image\n",
    "#         for c in range(3):\n",
    "#             blended_image_np[y_offset:y_offset+smaller_h, x_offset:x_offset+smaller_w, c] = (\n",
    "#                 alpha * smaller_image_np[:, :, c] +\n",
    "#                 (1 - alpha) * larger_image_np[y_offset:y_offset+smaller_h, x_offset:x_offset+smaller_w, c]\n",
    "#             )\n",
    "\n",
    "#     # Convert blended image back to PIL Image\n",
    "#     blended_image = Image.fromarray(blended_image_np.astype('uint8'))\n",
    "\n",
    "#     # Save blended image to output directory\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     output_file = os.path.join(output_dir, 'blended_image.png')\n",
    "#     blended_image.save(output_file)\n",
    "\n",
    "#     # Create JSON response\n",
    "#     json_data = {\n",
    "#         'output_path': output_file,\n",
    "#         'positions': positions,\n",
    "#         'alpha': alpha\n",
    "#     }\n",
    "\n",
    "#     # Write JSON to file\n",
    "#     json_output_file = os.path.join(output_dir, 'output.json')\n",
    "#     with open(json_output_file, 'w') as f:\n",
    "#         json.dump(json_data, f, indent=4)\n",
    "\n",
    "#     return json_output_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "def parse_positions(positions_str):\n",
    "    \"\"\"\n",
    "    Parses a string of positions into a list of tuples.\n",
    "\n",
    "    Parameters:\n",
    "    - positions_str (str): String representation of positions, e.g., \"[(10, 10), (100, 50), (200, 100)]\"\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: List of (x, y) positions.\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    positions_str = positions_str.strip()[1:-1]  # Remove the outer brackets\n",
    "    for pos in positions_str.split('),'):\n",
    "        pos = pos.replace('(', '').replace(')', '').strip()\n",
    "        x, y = map(int, pos.split(','))\n",
    "        positions.append((x, y))\n",
    "    return positions\n",
    "\n",
    "def blend_images(positions_str: str, alpha: float = 0.5, output_dir: str = './output') -> str:\n",
    "    \"\"\"\n",
    "    Blends multiple small images by placing them on top of a larger image at specified positions without overlapping and saves the blended image.\n",
    "\n",
    "    Parameters:\n",
    "    - positions_str (str): String representation of positions, e.g., \"[(20, 20), (120, 60), (220, 120), (320, 180), (420, 240), (520, 300)]\"\n",
    "    - alpha (float): Blending factor for transparency (0.0 to 1.0). Default is 0.5.\n",
    "    - output_dir (str): Directory where the blended image will be saved. Default is './output'.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the JSON file containing the output image information.\n",
    "    \"\"\"\n",
    "    # Define paths to small images (example paths)\n",
    "    small_images_paths = [\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/cta.jpg',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/discover.png',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_3.png',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_animation_1.png',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/engagement_instruction_1.png',\n",
    "        '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/landing_endframe.jpg'\n",
    "    ]\n",
    "\n",
    "    # Parse positions string\n",
    "    positions = parse_positions(positions_str)\n",
    "\n",
    "    if len(small_images_paths) != len(positions):\n",
    "        raise ValueError(\"The number of small images must match the number of positions.\")\n",
    "\n",
    "    # Load larger image from file path (example path)\n",
    "    larger_image_path = '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_1.jpg'\n",
    "    larger_image = Image.open(larger_image_path)\n",
    "    larger_image_np = np.array(larger_image)\n",
    "    \n",
    "    # Get dimensions of the larger image\n",
    "    larger_h, larger_w = larger_image_np.shape[:2]\n",
    "\n",
    "    blended_image_np = larger_image_np.copy()\n",
    "\n",
    "    # Blend each small image at its respective position\n",
    "    for i, small_image_path in enumerate(small_images_paths):\n",
    "        # Load small image from file path\n",
    "        smaller_image = Image.open(small_image_path)\n",
    "        smaller_image_np = np.array(smaller_image)\n",
    "\n",
    "        # Get dimensions of the smaller image\n",
    "        smaller_h, smaller_w = smaller_image_np.shape[:2]\n",
    "\n",
    "        # Resize smaller image if necessary\n",
    "        if smaller_h > larger_h or smaller_w > larger_w:\n",
    "            aspect_ratio = smaller_w / smaller_h\n",
    "            if smaller_h > larger_h:\n",
    "                smaller_h = larger_h\n",
    "                smaller_w = int(smaller_h * aspect_ratio)\n",
    "            if smaller_w > larger_w:\n",
    "                smaller_w = larger_w\n",
    "                smaller_h = int(smaller_w / aspect_ratio)\n",
    "            smaller_image = smaller_image.resize((smaller_w, smaller_h))\n",
    "            smaller_image_np = np.array(smaller_image)\n",
    "\n",
    "        x_offset, y_offset = positions[i]\n",
    "\n",
    "        # Ensure the small image does not go out of bounds\n",
    "        if x_offset + smaller_w > larger_w:\n",
    "            x_offset = larger_w - smaller_w\n",
    "        if y_offset + smaller_h > larger_h:\n",
    "            y_offset = larger_h - smaller_h\n",
    "\n",
    "        # Blend images\n",
    "        if smaller_image_np.ndim == 2:  # Grayscale image\n",
    "            smaller_image_np = np.stack([smaller_image_np] * 3, axis=-1)  # Convert to 3-channel image\n",
    "        for c in range(3):\n",
    "            blended_image_np[y_offset:y_offset+smaller_h, x_offset:x_offset+smaller_w, c] = (\n",
    "                alpha * smaller_image_np[:, :, c] +\n",
    "                (1 - alpha) * larger_image_np[y_offset:y_offset+smaller_h, x_offset:x_offset+smaller_w, c]\n",
    "            )\n",
    "\n",
    "    # Convert blended image back to PIL Image\n",
    "    blended_image = Image.fromarray(blended_image_np.astype('uint8'))\n",
    "\n",
    "    # Save blended image to output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'blended_image.png')\n",
    "    blended_image.save(output_file)\n",
    "\n",
    "    # Create JSON response\n",
    "    json_data = {\n",
    "        'output_path': output_file,\n",
    "        'positions': positions,\n",
    "        'alpha': alpha\n",
    "    }\n",
    "\n",
    "    # Write JSON to file\n",
    "    json_output_file = os.path.join(output_dir, 'output.json')\n",
    "    with open(json_output_file, 'w') as f:\n",
    "        json.dump(json_data, f, indent=4)\n",
    "\n",
    "    return json_output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/output.json'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = '[(20, 20), (120, 60), (220, 120), (320, 180), (420, 240), (520, 300)]'\n",
    "blend_images(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# def blend_images(position: str = 'center', alpha: float = 0.5, output_dir: str = './output') -> str:\n",
    "#     \"\"\"\n",
    "#     Blends two images by placing the smaller image on top of the larger image at a specified position and saves the blended image.\n",
    "\n",
    "#     Parameters:\n",
    "#     - position (str): Position where the smaller image will be placed on the larger image.\n",
    "#                       Options are 'center', 'top-left', 'top-right', 'bottom-left', 'bottom-right'. Default is 'center'.\n",
    "#     - alpha (float): Blending factor for transparency (0.0 to 1.0). Default is 0.5.\n",
    "#     - output_dir (str): Directory where the blended image will be saved. Default is './output'.\n",
    "\n",
    "#     Returns:\n",
    "#     - str: Path to the JSON file containing the output image information.\n",
    "#     \"\"\"\n",
    "#     larger_image_path = '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/endframe_1.jpg'\n",
    "#     smaller_image_path = '/home/jabez_kassa/week_12_updated/Semantic-Image-and-Text-Alignment/data/Assets/015efcdd8de3698ffc4dad6dabd6664a/cta.jpg'\n",
    "\n",
    "#     # Load images from file paths\n",
    "#     larger_image = Image.open(larger_image_path)\n",
    "#     smaller_image = Image.open(smaller_image_path)\n",
    "\n",
    "#     # Convert images to NumPy arrays\n",
    "#     larger_image_np = np.array(larger_image)\n",
    "#     smaller_image_np = np.array(smaller_image)\n",
    "\n",
    "#     # Get dimensions\n",
    "#     larger_h, larger_w = larger_image_np.shape[:2]\n",
    "#     smaller_h, smaller_w = smaller_image_np.shape[:2]\n",
    "\n",
    "#     # Resize smaller image if necessary\n",
    "#     if smaller_h > larger_h or smaller_w > larger_w:\n",
    "#         aspect_ratio = smaller_w / smaller_h\n",
    "#         if smaller_h > larger_h:\n",
    "#             smaller_h = larger_h\n",
    "#             smaller_w = int(smaller_h * aspect_ratio)\n",
    "#         if smaller_w > larger_w:\n",
    "#             smaller_w = larger_w\n",
    "#             smaller_h = int(smaller_w / aspect_ratio)\n",
    "#         smaller_image = smaller_image.resize((smaller_w, smaller_h))\n",
    "#         smaller_image_np = np.array(smaller_image)\n",
    "\n",
    "#     # Determine position\n",
    "#     positions = {\n",
    "#         'center': ((larger_w - smaller_w) // 2, (larger_h - smaller_h) // 2),\n",
    "#         'top-left': (0, 0),\n",
    "#         'top-right': (larger_w - smaller_w, 0),\n",
    "#         'bottom-left': (0, larger_h - smaller_h),\n",
    "#         'bottom-right': (larger_w - smaller_w, larger_h - smaller_h)\n",
    "#     }\n",
    "\n",
    "#     if position not in positions:\n",
    "#         raise ValueError(\"Invalid position argument. Choose from 'center', 'top-left', 'top-right', 'bottom-left', 'bottom-right'\")\n",
    "    \n",
    "#     x_offset, y_offset = positions[position]\n",
    "\n",
    "#     # Blend images\n",
    "#     blended_image_np = larger_image_np.copy()\n",
    "#     for c in range(3):\n",
    "#         blended_image_np[y_offset:y_offset+smaller_h, x_offset:x_offset+smaller_w, c] = (\n",
    "#             alpha * smaller_image_np[:, :, c] +\n",
    "#             (1 - alpha) * larger_image_np[y_offset:y_offset+smaller_h, x_offset:x_offset+smaller_w, c]\n",
    "#         )\n",
    "\n",
    "#     # Convert blended image back to PIL Image\n",
    "#     blended_image = Image.fromarray(blended_image_np.astype('uint8'))\n",
    "\n",
    "#     # Save blended image to output directory\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     output_file = os.path.join(output_dir, 'blended_image.png')\n",
    "#     blended_image.save(output_file)\n",
    "\n",
    "#     # Create JSON response\n",
    "#     json_data = {\n",
    "#         'output_path': output_file,\n",
    "#         'position': position,\n",
    "#         'alpha': alpha\n",
    "#     }\n",
    "\n",
    "#     # Write JSON to file\n",
    "#     json_output_file = os.path.join(output_dir, 'output.json')\n",
    "#     with open(json_output_file, 'w') as f:\n",
    "#         json.dump(json_data, f, indent=4)\n",
    "\n",
    "#     return json_output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config2 = {\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": openai_api_key}]}\n",
    "code_execution_config = {\"use_docker\": False}\n",
    "\n",
    "# Initialize the assistant agent with the given configurations\n",
    "config_list = [\n",
    "    {\"model\": \"gpt-4\", \"api_key\": openai_api_key, \"api_type\": \"openai\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config={\n",
    "        \"temperature\": 0,\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"functions\": [\n",
    "             {\n",
    "                        \"name\": \"blend_images\",\n",
    "                        \"description\": \"use this function to blend the images\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"positions_str\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"This is where you will position the blending\"\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"positions_str\"]\n",
    "                        }\n",
    "                        },\n",
    "             {\n",
    "                        \"name\": \"read_img\",\n",
    "                        \"description\": \"use this to read the image blended from blended image\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"output_json\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"description\": \"This is the blended image\"\n",
    "                                },\n",
    "                            },\n",
    "                            \"required\": [\"position\"]\n",
    "                        }\n",
    "                        }\n",
    "                        ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "# Let's first define the assistant agent that suggests tool calls.\n",
    "img_blend_assistant = autogen.AssistantAgent(\n",
    "    name=\"image_blending_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\n",
    "    \n",
    "\"\"\"You are a helpful AI assistant. \n",
    "The main problems you will be solving include:\n",
    "- suggest diffrent \"positions\" to make a good advertising \n",
    "    use this as example: \"positions_str = [(10, 10), (100, 50), (200, 100), (300, 150), (400, 200), (500, 250)]\"\n",
    "    make sure you are giving only 6 positions\n",
    "    make sure that the images will not overlap\n",
    "- This are the discription of the pictures:\n",
    "    1. the first position is for a picture that have a text saying 'shop now '\n",
    "    2. the second position is for a picture that have a text saying 'Discover 12 unique tea flavours delivered to your door'\n",
    "    3. the third position is for a picture that have a text saying 'Enjoy tea delivered to your home'\n",
    "    4. the fourth position is for a picture that shows a hand pointing\n",
    "    5. the fifth position is for a picture that have a text saying 'tap to get letter box delivery of tea'\n",
    "    6. the sixth position is for a picture that have a text saying 'off black generation picture'\n",
    "\n",
    "- your task:\n",
    "    - considering the above discription for each picture find a way to position each picture to give a good advertaizing\n",
    "    - Then modifay the position after the comment from 'img_critic_assistant'\n",
    "    \"\"\"\n",
    "    ,\n",
    "    llm_config=llm_config,\n",
    "    function_map={\n",
    "        \"blend_images\": blend_images,\n",
    "        \"read_img\": read_img\n",
    "    }\n",
    "        \n",
    ")\n",
    "\n",
    "img_critic_assistant = autogen.AssistantAgent(\n",
    "    name=\"image_blending_assistant\",\n",
    "    code_execution_config=False,\n",
    "    system_message=\"You are a advertizing image critic AI assistant. \"\n",
    "    \"\"\"You task is to critic the 'output.json' from 'img_blend_assistant'\n",
    "    critic the following part \n",
    "  \n",
    "                            - the first position is for a picture that have a text saying 'shop now '\n",
    "                            - the second position is for a picture that have a text saying 'Discover 12 unique tea flavours delivered to your door'\n",
    "                            - the third position is for a picture that have a text saying 'Enjoy tea delivered to your home'\n",
    "                            - the fourth position is for a picture that shows a hand pointing\n",
    "                            - the fifth position is for a picture that have a text saying 'tap to get letter box delivery of tea'\n",
    "                            - the sixth position is for a picture that have a text saying 'off black generation picture'\n",
    "    recomend 'img_blend_assistant' for a better advertising.\n",
    "    \"Return 'TERMINATE' when the task is done.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    function_map={\n",
    "        \"blend_images\": blend_images,\n",
    "        \"read_img\": read_img\n",
    "    }\n",
    "        \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The user proxy agent is used for interacting with the assistant agent\n",
    "# and executes tool calls.\n",
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False\n",
    "    # # is_termination_msg=lambda x: \"content\" in x and x[\"content\"] is not None and x[\"content\"].rstrip().endswith(\"TERMINATE\"),\n",
    "    # # code_execution_config={\"work_dir\": \"planning\"},\n",
    "    # function_map={\"blend_images\": blend_images},\n",
    "    # code_execution_config=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, img_blend_assistant, img_critic_assistant],\n",
    " \n",
    "    messages=[],  # The initial messages in the chat\n",
    "    max_round=10,  # Maximum rounds of conversation\n",
    "    select_speaker_message_template = \n",
    "    \"\"\" This is the flow of the converstion:\n",
    "    1. 'user_proxy'\n",
    "    2. 'img_blend_assistant'\n",
    "    3. 'img_critic_assistant'\n",
    "    4. 'img_blend_assistant'\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "blend the images at diffrent positions\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image_blending_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function call: blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"positions_str\": \"[(10, 10), (100, 50), (200, 100), (300, 150), (400, 200), (500, 250)]\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image_blending_assistant\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function (blend_images) *****\u001b[0m\n",
      "./output/output.json\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image_blending_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "The images have been blended at the specified positions. Now, let's read the blended image to evaluate the result.\n",
      "\u001b[32m***** Suggested function call: read_img *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m*********************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image_blending_assistant\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION read_img...\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function (read_img) *****\u001b[0m\n",
      "Error: read_img() missing 1 required positional argument: 'json_path'\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image_blending_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the mistake. Let's try reading the image again with the correct parameters.\n",
      "\u001b[32m***** Suggested function call: read_img *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"json_path\": \"./output/output.json\"\n",
      "}\n",
      "\u001b[32m*********************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image_blending_assistant\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION read_img...\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function (read_img) *****\u001b[0m\n",
      "The image features the promotion of a tea delivery service. The primary elements include:\n",
      "\n",
      "1. A partially visible electric kettle with a sleek, modern design in a light color.\n",
      "2. A transparent double-walled glass containing a tea bag steeping in what appears to be a pinkish-color liquid. The tea bag has a label reading \"OFFBLAK\".\n",
      "3. The brand name \"OFFBLAK\" and the phrase \"GENERATION T\" are prominently displayed across the image.\n",
      "4. Text at the top of the image says \"Discover 12 unique tea flavours delivered to your door.\"\n",
      "5. Text towards the bottom of the image reads \"Enjoy tea delivered to your door.\"\n",
      "6. There is a \"SHOP NOW\" button at the top left of the image.\n",
      "\n",
      "The overall aesthetic is minimalistic and clean, highlighting the convenience and modern appeal of having tea delivered to one's home.\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image_blending_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the image description, here are my critiques and recommendations for a better advertising:\n",
      "\n",
      "1. The 'Shop Now' text is placed at the top left of the image. This is not an ideal position as it is not the first place viewers typically look. It would be better to place this call-to-action button in a more prominent position, such as the center or bottom right of the image.\n",
      "\n",
      "2. The text 'Discover 12 unique tea flavours delivered to your door' and 'Enjoy tea delivered to your door' seem to be repetitive. It would be more effective to combine these messages or use different phrases to highlight the variety of teas and the convenience of delivery.\n",
      "\n",
      "3. The image of the hand pointing is not mentioned in the description. If it is present, it should be used effectively to guide the viewer's attention towards key elements such as the 'Shop Now' button or the unique selling points of the product.\n",
      "\n",
      "4. The 'tap to get letter box delivery of tea' text is not mentioned in the description. If it is present, it should be placed near the 'Shop Now' button to encourage immediate action.\n",
      "\n",
      "5. The 'off black generation picture' text is not clear. If it refers to the brand name and slogan, they are well-placed and prominent. However, if it is a separate element, it should be clarified or made more visible.\n",
      "\n",
      "6. The image elements like the electric kettle and the tea in the glass are well-chosen to represent the product. However, they could be arranged in a more engaging way to tell a story or create a more dynamic composition.\n",
      "\n",
      "7. The overall aesthetic is clean and modern, which is good. However, adding some warmth or personal touch could make the ad more appealing and relatable to the target audience.\n",
      "\n",
      "Please consider these points for improving the advertisement. If you need further assistance, feel free to ask.\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image_blending_assistant\n",
      "\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the feedback, let's try repositioning the images. Here are the new positions:\n",
      "\n",
      "1. 'Shop Now' - Move this to the center of the image to make it more prominent. New position: (250, 250)\n",
      "2. 'Discover 12 unique tea flavours delivered to your door' - Keep this at the top but move it slightly to the right for balance. New position: (150, 10)\n",
      "3. 'Enjoy tea delivered to your door' - Move this to the bottom right to balance the composition. New position: (400, 400)\n",
      "4. Hand pointing - If present, position this near the 'Shop Now' button to guide viewers' attention. New position: (300, 250)\n",
      "5. 'Tap to get letter box delivery of tea' - Position this near the 'Shop Now' button to encourage action. New position: (250, 300)\n",
      "6. 'Off black generation picture' - If this is a separate element, position it at the top right for visibility. New position: (500, 10)\n",
      "\n",
      "Let's blend the images at these new positions.\n",
      "\u001b[32m***** Suggested function call: blend_images *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"positions_str\": \"[(250, 250), (150, 10), (400, 400), (300, 250), (250, 300), (500, 10)]\"\n",
      "}\n",
      "\u001b[32m*************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: image_blending_assistant\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION blend_images...\u001b[0m\n",
      "\u001b[33mimage_blending_assistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function (blend_images) *****\u001b[0m\n",
      "./output/output.json\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'blend the images at diffrent positions', 'role': 'assistant'}, {'content': '', 'function_call': {'arguments': '{\\n\"positions_str\": \"[(10, 10), (100, 50), (200, 100), (300, 150), (400, 200), (500, 250)]\"\\n}', 'name': 'blend_images'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': './output/output.json', 'name': 'blend_images', 'role': 'function'}, {'content': \"The images have been blended at the specified positions. Now, let's read the blended image to evaluate the result.\", 'function_call': {'arguments': '{}', 'name': 'read_img'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': \"Error: read_img() missing 1 required positional argument: 'json_path'\", 'name': 'read_img', 'role': 'function'}, {'content': \"I apologize for the mistake. Let's try reading the image again with the correct parameters.\", 'function_call': {'arguments': '{\\n\"json_path\": \"./output/output.json\"\\n}', 'name': 'read_img'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': 'The image features the promotion of a tea delivery service. The primary elements include:\\n\\n1. A partially visible electric kettle with a sleek, modern design in a light color.\\n2. A transparent double-walled glass containing a tea bag steeping in what appears to be a pinkish-color liquid. The tea bag has a label reading \"OFFBLAK\".\\n3. The brand name \"OFFBLAK\" and the phrase \"GENERATION T\" are prominently displayed across the image.\\n4. Text at the top of the image says \"Discover 12 unique tea flavours delivered to your door.\"\\n5. Text towards the bottom of the image reads \"Enjoy tea delivered to your door.\"\\n6. There is a \"SHOP NOW\" button at the top left of the image.\\n\\nThe overall aesthetic is minimalistic and clean, highlighting the convenience and modern appeal of having tea delivered to one\\'s home.', 'name': 'read_img', 'role': 'function'}, {'content': \"Based on the image description, here are my critiques and recommendations for a better advertising:\\n\\n1. The 'Shop Now' text is placed at the top left of the image. This is not an ideal position as it is not the first place viewers typically look. It would be better to place this call-to-action button in a more prominent position, such as the center or bottom right of the image.\\n\\n2. The text 'Discover 12 unique tea flavours delivered to your door' and 'Enjoy tea delivered to your door' seem to be repetitive. It would be more effective to combine these messages or use different phrases to highlight the variety of teas and the convenience of delivery.\\n\\n3. The image of the hand pointing is not mentioned in the description. If it is present, it should be used effectively to guide the viewer's attention towards key elements such as the 'Shop Now' button or the unique selling points of the product.\\n\\n4. The 'tap to get letter box delivery of tea' text is not mentioned in the description. If it is present, it should be placed near the 'Shop Now' button to encourage immediate action.\\n\\n5. The 'off black generation picture' text is not clear. If it refers to the brand name and slogan, they are well-placed and prominent. However, if it is a separate element, it should be clarified or made more visible.\\n\\n6. The image elements like the electric kettle and the tea in the glass are well-chosen to represent the product. However, they could be arranged in a more engaging way to tell a story or create a more dynamic composition.\\n\\n7. The overall aesthetic is clean and modern, which is good. However, adding some warmth or personal touch could make the ad more appealing and relatable to the target audience.\\n\\nPlease consider these points for improving the advertisement. If you need further assistance, feel free to ask.\\nTERMINATE\", 'name': 'image_blending_assistant', 'role': 'user'}, {'content': \"Based on the feedback, let's try repositioning the images. Here are the new positions:\\n\\n1. 'Shop Now' - Move this to the center of the image to make it more prominent. New position: (250, 250)\\n2. 'Discover 12 unique tea flavours delivered to your door' - Keep this at the top but move it slightly to the right for balance. New position: (150, 10)\\n3. 'Enjoy tea delivered to your door' - Move this to the bottom right to balance the composition. New position: (400, 400)\\n4. Hand pointing - If present, position this near the 'Shop Now' button to guide viewers' attention. New position: (300, 250)\\n5. 'Tap to get letter box delivery of tea' - Position this near the 'Shop Now' button to encourage action. New position: (250, 300)\\n6. 'Off black generation picture' - If this is a separate element, position it at the top right for visibility. New position: (500, 10)\\n\\nLet's blend the images at these new positions.\", 'function_call': {'arguments': '{\\n\"positions_str\": \"[(250, 250), (150, 10), (400, 400), (300, 250), (250, 300), (500, 10)]\"\\n}', 'name': 'blend_images'}, 'name': 'image_blending_assistant', 'role': 'assistant'}, {'content': './output/output.json', 'name': 'blend_images', 'role': 'function'}], summary='./output/output.json', cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager, message=\"blend the images at diffrent positions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
